import time
import argparse

import tensorflow as tf
import horovod.tensorflow.keras as hvd

from tensorflow.keras import layers, Model
from tensorflow.keras import applications
from tensorflow.keras.applications import VGG19, Xception, ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dropout, Flatten, Dense, UpSampling2D

# Initialize Horovod
hvd.init()

# Horovod: pin GPU to be used to process local rank (one GPU per process)
gpus = tf.config.experimental.list_physical_devices('GPU')
print("Num GPUs Available: ", len(gpus))

for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
if gpus:
    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
    
(cifar10_images, cifar10_labels), _ = tf.keras.datasets.cifar10.load_data()

dataset = tf.data.Dataset.from_tensor_slices(
    (tf.cast(cifar10_images[..., tf.newaxis] / 255.0, tf.float32),
             tf.cast(cifar10_labels, tf.int64))
)
dataset = dataset.repeat().shuffle(10000).batch(batch_size)

def model(model_name, epochs, batch_size, learning_rate, steps_per_epoch):
  if(model_name == 'VGG19'):
    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(32, 32, 3))

    model = Sequential()
    model.add(base_model)
    model.add(Flatten())
    model.add(Dense(256))
    model.add(Dense(64))
    model.add(Dense(10, activation='softmax'))
  elif(model_name == 'ResNet50'):
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))

    model = Sequential()
    model.add(base_model)
    model.add(Flatten())
    model.add(Dense(256))
    model.add(Dense(64))
    model.add(Dense(10, activation='softmax'))
  else:
    return print("Model not found.")

  # Horovod: adjust learning rate based on number of GPUs.
  scaled_lr = learning_rate * hvd.size()
  opt = tf.optimizers.Adam(scaled_lr)

  # Horovod: add Horovod DistributedOptimizer.
  opt = hvd.DistributedOptimizer(
      opt, backward_passes_per_step=1, average_aggregated_gradients=True)

  # Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow
  # uses hvd.DistributedOptimizer() to compute gradients.
  model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),
                    optimizer=opt,
                    metrics=['accuracy'],
                    experimental_run_tf_function=False) # uses hvd.DistributedOptimizer()

  callbacks = [
      # Horovod: broadcast initial variable states from rank 0 to all other processes.
      # This is necessary to ensure consistent initialization of all workers when
      # training is started with random weights or restored from a checkpoint.
      hvd.callbacks.BroadcastGlobalVariablesCallback(0),

      # Horovod: average metrics among workers at the end of every epoch.
      #
      # Note: This callback must be in the list before the ReduceLROnPlateau,
      # TensorBoard or other metrics-based callbacks.
      hvd.callbacks.MetricAverageCallback(),

      # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final
      # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during
      # the first three epochs. See https://arxiv.org/abs/1706.02677 for details.
      hvd.callbacks.LearningRateWarmupCallback(initial_lr=scaled_lr, warmup_epochs=3, verbose=1),
  ]

  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.
  if hvd.rank() == 0:
      callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))

  # Horovod: write logs on worker 0.
  verbose = 1 if hvd.rank() == 0 else 0


  print("Model: {}, Epochs: {}".format(model_name, epochs))
  training_start = time.time()

  # Train the model.
  # Horovod: adjust number of steps based on number of GPUs.
  history = model.fit(dataset, steps_per_epoch=steps_per_epoch // hvd.size(), callbacks=callbacks, epochs=epochs, verbose=verbose)

  training_end = (time.time() - training_start)


  #print("--------------Test performance--------------")
  #test_loss, test_acc = model.evaluate(X_test / 255.0, Y_test, verbose = 2)

  log_name = "Horovod_TEST2_Cifar10_GPU_{}_{}.txt".format(model_name, batch_size)
  with open(log_name, "w") as f: 
    f.write("Training Time: "+ str(training_end) + "\n")
    f.write("Epochs: "+ str(epochs) + "\n")
    f.write("Steps per Epoch: "+ str(steps_per_epoch) + "\n")
    f.write("Accuracy: "+ str(history.history['accuracy']) + "\n")
    
#Batch Size 256
epochs = 5
batch_size = 256
learning_rate = 1e-3
steps_per_epoch = 500

model("VGG19", epochs, batch_size, learning_rate, steps_per_epoch)
#model("ResNet50", epochs, batch_size, learning_rate, steps_per_epoch)
